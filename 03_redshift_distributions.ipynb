{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Redshift Distributions and Cosmological Models\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Understanding redshift distributions is crucial for weak lensing analysis because:\n",
    "\n",
    "1. **Lensing efficiency** depends on the distance to both lens and source\n",
    "2. **Redshift binning** allows analysis of different galaxy populations\n",
    "3. **Cosmological modeling** requires accurate distance-redshift relations\n",
    "\n",
    "In this tutorial, you'll learn to work with galaxy redshift distributions and set up cosmological models for lensing calculations.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "1. Extract and analyze redshift distributions from galaxy catalogues\n",
    "2. Use Gaussian Mixture Models (GMM) to bin galaxies by redshift\n",
    "3. Understand the relationship between redshift and lensing efficiency\n",
    "4. Set up cosmological models using PyCC\n",
    "5. Calculate weak lensing kernels and efficiency functions\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Completion of Tutorials 1 and 2\n",
    "- Basic understanding of cosmological distances\n",
    "- Familiarity with Gaussian distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import norm\n",
    "import healpy as hp\n",
    "\n",
    "# Try to import cosmological tools\n",
    "try:\n",
    "    import pyccl as ccl\n",
    "    print('✅ PyCC (Core Cosmology Library) imported successfully')\n",
    "    CCL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print('⚠️  PyCC not found. Install with: pip install pyccl')\n",
    "    print('   Some cosmological calculations will be simulated')\n",
    "    CCL_AVAILABLE = False\n",
    "\n",
    "# Set up plotting\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print('🌌 Tutorial 3: Redshift Distributions and Cosmology')\n",
    "print('📦 All packages imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading and Examining Redshift Data\n",
    "\n",
    "Let's start by loading our galaxy catalogue and examining its redshift distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load galaxy catalogue\n",
    "try:\n",
    "    catalogue = np.load(\"catalogue.npy\", allow_pickle=True)\n",
    "    print(f\"📁 Loaded catalogue with {len(catalogue):,} galaxies\")\n",
    "    \n",
    "    # Extract redshift data\n",
    "    z_data = catalogue['z']\n",
    "    \n",
    "    # Basic redshift statistics\n",
    "    print(f\"\\n📊 Redshift Statistics:\")\n",
    "    print(f\"  Range: {z_data.min():.3f} - {z_data.max():.3f}\")\n",
    "    print(f\"  Mean: {z_data.mean():.3f}\")\n",
    "    print(f\"  Median: {np.median(z_data):.3f}\")\n",
    "    print(f\"  Standard deviation: {z_data.std():.3f}\")\n",
    "    \n",
    "    CATALOGUE_AVAILABLE = True\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ catalogue.npy not found. Creating synthetic redshift data for demonstration.\")\n",
    "    \n",
    "    # Create synthetic redshift distribution for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_galaxies = 50000\n",
    "    \n",
    "    # Multi-component redshift distribution (common in real surveys)\n",
    "    z1 = np.random.normal(0.3, 0.1, int(0.4 * n_galaxies))  # Low-z peak\n",
    "    z2 = np.random.normal(0.7, 0.2, int(0.4 * n_galaxies))  # Mid-z peak\n",
    "    z3 = np.random.exponential(0.3, int(0.2 * n_galaxies)) + 1.0  # High-z tail\n",
    "    \n",
    "    z_data = np.concatenate([z1, z2, z3])\n",
    "    z_data = z_data[z_data > 0]  # Remove negative redshifts\n",
    "    z_data = z_data[z_data < 3.0]  # Set reasonable upper limit\n",
    "    \n",
    "    print(f\"📁 Created synthetic redshift data with {len(z_data):,} galaxies\")\n",
    "    print(f\"  Range: {z_data.min():.3f} - {z_data.max():.3f}\")\n",
    "    print(f\"  Mean: {z_data.mean():.3f}\")\n",
    "    \n",
    "    CATALOGUE_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Redshift Distribution\n",
    "\n",
    "The redshift distribution n(z) tells us how many galaxies we observe at each distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the redshift distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Histogram of redshifts\n",
    "ax1.hist(z_data, bins=100, alpha=0.7, color='skyblue', edgecolor='black', density=True)\n",
    "ax1.set_xlabel('Redshift z')\n",
    "ax1.set_ylabel('Probability Density n(z)')\n",
    "ax1.set_title('Galaxy Redshift Distribution')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add some statistics to the plot\n",
    "ax1.axvline(np.mean(z_data), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(z_data):.3f}')\n",
    "ax1.axvline(np.median(z_data), color='orange', linestyle='--', linewidth=2, label=f'Median: {np.median(z_data):.3f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Cumulative distribution\n",
    "z_sorted = np.sort(z_data)\n",
    "cumulative = np.arange(1, len(z_sorted) + 1) / len(z_sorted)\n",
    "ax2.plot(z_sorted, cumulative, linewidth=2, color='green')\n",
    "ax2.set_xlabel('Redshift z')\n",
    "ax2.set_ylabel('Cumulative Fraction')\n",
    "ax2.set_title('Cumulative Redshift Distribution')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add percentile lines\n",
    "percentiles = [25, 50, 75]\n",
    "for p in percentiles:\n",
    "    z_p = np.percentile(z_data, p)\n",
    "    ax2.axhline(p/100, color='red', linestyle=':', alpha=0.7)\n",
    "    ax2.axvline(z_p, color='red', linestyle=':', alpha=0.7)\n",
    "    ax2.text(z_p + 0.05, p/100 - 0.05, f'{p}%', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n🔍 Key percentiles:\")\n",
    "for p in [10, 25, 50, 75, 90]:\n",
    "    print(f\"  {p}th percentile: z = {np.percentile(z_data, p):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gaussian Mixture Model for Redshift Binning\n",
    "\n",
    "We'll use a Gaussian Mixture Model (GMM) to identify distinct redshift populations and create redshift bins. This is useful for:\n",
    "\n",
    "1. **Tomographic analysis**: Studying lensing at different distances\n",
    "2. **Population identification**: Separating different galaxy types\n",
    "3. **Improved modeling**: Accounting for complex redshift distributions\n",
    "\n",
    "### Setting Up the GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Gaussian Mixture Model\n",
    "n_components = 5  # Number of Gaussian components\n",
    "\n",
    "# Reshape data for sklearn (needs 2D array)\n",
    "data = z_data.reshape(-1, 1)\n",
    "\n",
    "# Define initial means for better convergence\n",
    "# Space them evenly across the redshift range\n",
    "z_min, z_max = z_data.min(), z_data.max()\n",
    "initial_means = np.linspace(z_min + 0.1, z_max - 0.1, n_components).reshape(-1, 1)\n",
    "\n",
    "print(f\"🧮 Setting up Gaussian Mixture Model:\")\n",
    "print(f\"  Number of components: {n_components}\")\n",
    "print(f\"  Initial means: {initial_means.flatten()}\")\n",
    "\n",
    "# Fit GMM with manually initialized means\n",
    "gmm = GaussianMixture(n_components=n_components, \n",
    "                     means_init=initial_means, \n",
    "                     random_state=42,\n",
    "                     max_iter=200)\n",
    "\n",
    "print(f\"\\n🔄 Fitting GMM to {len(data):,} galaxies...\")\n",
    "gmm.fit(data)\n",
    "\n",
    "print(f\"✅ GMM fit completed!\")\n",
    "print(f\"  Converged: {gmm.converged_}\")\n",
    "print(f\"  Number of iterations: {gmm.n_iter_}\")\n",
    "print(f\"  Log-likelihood: {gmm.score(data):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing GMM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract GMM parameters\n",
    "means = gmm.means_.flatten()\n",
    "covariances = gmm.covariances_.flatten()\n",
    "weights = gmm.weights_\n",
    "\n",
    "print(f\"📊 GMM Component Analysis:\")\n",
    "print(f\"{'Component':<12} {'Mean z':<10} {'Std Dev':<10} {'Weight':<10} {'Fraction':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i in range(n_components):\n",
    "    std_dev = np.sqrt(covariances[i])\n",
    "    fraction = weights[i] * 100\n",
    "    print(f\"{i+1:<12} {means[i]:<10.3f} {std_dev:<10.3f} {weights[i]:<10.3f} {fraction:<12.1f}%\")\n",
    "\n",
    "# Get component assignments for each galaxy\n",
    "probabilities = gmm.predict_proba(data)  # Probability of belonging to each component\n",
    "labels = gmm.predict(data)               # Most likely component\n",
    "\n",
    "# For soft assignment (probabilistic), we can also randomly assign based on probabilities\n",
    "soft_labels = np.array([np.random.choice(n_components, p=prob) for prob in probabilities])\n",
    "\n",
    "print(f\"\\n🏷️  Galaxy Assignment:\")\n",
    "unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"  Component {label+1}: {count:,} galaxies ({count/len(data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the GMM Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of GMM fit\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Original data with GMM components\n",
    "z_range = np.linspace(z_data.min(), z_data.max(), 1000).reshape(-1, 1)\n",
    "\n",
    "# Plot original histogram\n",
    "ax1.hist(z_data, bins=80, alpha=0.6, density=True, color='lightgray', \n",
    "         edgecolor='black', label='Original Data')\n",
    "\n",
    "# Plot overall GMM fit\n",
    "total_density = np.exp(gmm.score_samples(z_range))\n",
    "ax1.plot(z_range, total_density, 'black', linewidth=3, label='GMM Fit')\n",
    "\n",
    "# Plot individual components\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "for i in range(n_components):\n",
    "    component_density = weights[i] * norm.pdf(z_range.flatten(), means[i], np.sqrt(covariances[i]))\n",
    "    ax1.plot(z_range, component_density, colors[i % len(colors)], linewidth=2, \n",
    "             linestyle='--', label=f'Component {i+1} (z̄={means[i]:.3f})')\n",
    "\n",
    "ax1.set_xlabel('Redshift z')\n",
    "ax1.set_ylabel('Probability Density')\n",
    "ax1.set_title('Gaussian Mixture Model Decomposition')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Galaxy assignments by component\n",
    "for i in range(n_components):\n",
    "    mask = labels == i\n",
    "    if np.any(mask):\n",
    "        ax2.hist(z_data[mask], bins=40, alpha=0.7, density=True,\n",
    "                color=colors[i % len(colors)], \n",
    "                label=f'Component {i+1} ({np.sum(mask):,} gal)')\n",
    "\n",
    "ax2.set_xlabel('Redshift z')\n",
    "ax2.set_ylabel('Probability Density')\n",
    "ax2.set_title('Galaxy Assignments by Component')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n💡 The GMM has successfully decomposed the redshift distribution into\")\n",
    "print(f\"   {n_components} components, representing different galaxy populations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Cosmological Modeling with PyCC\n",
    "\n",
    "Now we'll set up a cosmological model to calculate lensing quantities. This involves:\n",
    "\n",
    "1. **Defining cosmological parameters** (Ωₘ, Ωᵦ, h, σ₈, etc.)\n",
    "2. **Creating distance-redshift relations**\n",
    "3. **Setting up weak lensing tracers**\n",
    "4. **Computing angular power spectra**\n",
    "\n",
    "### Setting Up the Cosmology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cosmological parameters (typical Planck 2018 values)\n",
    "omega_m = 0.31      # Total matter density\n",
    "omega_b = 0.049     # Baryon density  \n",
    "omega_c = omega_m - omega_b  # Cold dark matter density\n",
    "h = 0.67            # Hubble parameter H₀ = 100h km/s/Mpc\n",
    "sigma8 = 0.81       # Amplitude of matter fluctuations\n",
    "n_s = 0.96          # Spectral index\n",
    "\n",
    "print(f\"🌌 Cosmological Parameters:\")\n",
    "print(f\"  Ωₘ = {omega_m} (total matter)\")\n",
    "print(f\"  Ωᵦ = {omega_b} (baryons)\")\n",
    "print(f\"  Ωc = {omega_c} (cold dark matter)\")\n",
    "print(f\"  h = {h} (H₀ = {h*100} km/s/Mpc)\")\n",
    "print(f\"  σ₈ = {sigma8} (fluctuation amplitude)\")\n",
    "print(f\"  nₛ = {n_s} (spectral index)\")\n",
    "\n",
    "if CCL_AVAILABLE:\n",
    "    # Create CCL cosmology object\n",
    "    cosmo = ccl.Cosmology(\n",
    "        Omega_c=omega_c, \n",
    "        Omega_b=omega_b, \n",
    "        h=h, \n",
    "        sigma8=sigma8, \n",
    "        n_s=n_s\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ CCL Cosmology object created successfully\")\n",
    "    print(f\"  Ωₖ = {cosmo['Omega_k']:.4f} (curvature)\")\n",
    "    print(f\"  ΩΛ = {1 - cosmo['Omega_m']:.4f} (dark energy)\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n⚠️  CCL not available - will simulate cosmological calculations\")\n",
    "    cosmo = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Lensing Efficiency\n",
    "\n",
    "The lensing efficiency function describes how effective lensing is for galaxies at different redshifts. It depends on:\n",
    "\n",
    "- **Source redshift distribution**: Where the lensed galaxies are\n",
    "- **Lens redshift**: Where the lensing matter is\n",
    "- **Cosmological distances**: Angular diameter distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one redshift bin for detailed analysis\n",
    "selected_bin = 2  # Choose component with reasonable statistics\n",
    "bin_mask = labels == selected_bin\n",
    "bin_galaxies = z_data[bin_mask]\n",
    "\n",
    "print(f\"🔍 Analyzing Component {selected_bin + 1}:\")\n",
    "print(f\"  Number of galaxies: {np.sum(bin_mask):,}\")\n",
    "print(f\"  Mean redshift: {means[selected_bin]:.3f}\")\n",
    "print(f\"  Standard deviation: {np.sqrt(covariances[selected_bin]):.3f}\")\n",
    "print(f\"  Redshift range: {bin_galaxies.min():.3f} - {bin_galaxies.max():.3f}\")\n",
    "\n",
    "if CCL_AVAILABLE:\n",
    "    # Create redshift array for calculations\n",
    "    z_array = np.linspace(0.001, 3.0, 1000)\n",
    "    \n",
    "    # Create normalized redshift distribution for this bin\n",
    "    # Use the GMM component as the true distribution\n",
    "    nz = norm.pdf(z_array, means[selected_bin], np.sqrt(covariances[selected_bin]))\n",
    "    nz = nz / np.trapz(nz, z_array)  # Normalize\n",
    "    \n",
    "    # Create weak lensing tracer\n",
    "    lens_tracer = ccl.WeakLensingTracer(cosmo, dndz=(z_array, nz))\n",
    "    \n",
    "    print(f\"\\n✅ Created weak lensing tracer for component {selected_bin + 1}\")\n",
    "    \n",
    "else:\n",
    "    # Simulate the process\n",
    "    z_array = np.linspace(0.001, 3.0, 1000)\n",
    "    nz = norm.pdf(z_array, means[selected_bin], np.sqrt(covariances[selected_bin]))\n",
    "    nz = nz / np.trapz(nz, z_array)\n",
    "    lens_tracer = None\n",
    "    \n",
    "    print(f\"\\n⚠️  Simulating lensing tracer creation for component {selected_bin + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Redshift Distribution and Lensing Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the selected redshift bin and its properties\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Redshift distribution for selected bin\n",
    "ax1.hist(bin_galaxies, bins=30, alpha=0.7, density=True, \n",
    "         color='skyblue', edgecolor='black', label='Data')\n",
    "ax1.plot(z_array, nz, 'red', linewidth=2, label='GMM Component')\n",
    "ax1.set_xlabel('Redshift z')\n",
    "ax1.set_ylabel('n(z) [normalized]')\n",
    "ax1.set_title(f'Component {selected_bin + 1} Redshift Distribution')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Cumulative distribution\n",
    "cumulative_nz = np.cumsum(nz) * (z_array[1] - z_array[0])\n",
    "ax2.plot(z_array, cumulative_nz, 'green', linewidth=2)\n",
    "ax2.set_xlabel('Redshift z')\n",
    "ax2.set_ylabel('Cumulative n(z)')\n",
    "ax2.set_title('Cumulative Distribution')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Distance-redshift relation\n",
    "if CCL_AVAILABLE:\n",
    "    # Comoving distance\n",
    "    chi = ccl.comoving_radial_distance(cosmo, 1/(1+z_array))\n",
    "    ax3.plot(z_array, chi, 'blue', linewidth=2)\n",
    "    ax3.set_xlabel('Redshift z')\n",
    "    ax3.set_ylabel('Comoving Distance [Mpc]')\n",
    "    ax3.set_title('Distance-Redshift Relation')\n",
    "else:\n",
    "    # Approximate distance relation for demonstration\n",
    "    c = 299792.458  # km/s\n",
    "    H0 = h * 100    # km/s/Mpc\n",
    "    chi_approx = c/H0 * z_array  # Linear approximation\n",
    "    ax3.plot(z_array, chi_approx, 'blue', linewidth=2, linestyle='--')\n",
    "    ax3.set_xlabel('Redshift z')\n",
    "    ax3.set_ylabel('Distance [Mpc] (approx)')\n",
    "    ax3.set_title('Approximate Distance-Redshift')\n",
    "\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Lensing efficiency (simplified)\n",
    "# The lensing efficiency peaks when lenses are roughly halfway to sources\n",
    "z_lens = np.linspace(0, 2.5, 100)\n",
    "efficiency = np.zeros_like(z_lens)\n",
    "\n",
    "for i, zl in enumerate(z_lens):\n",
    "    # Sources behind this lens redshift\n",
    "    mask_behind = z_array > zl\n",
    "    if np.any(mask_behind):\n",
    "        # Weight by source distribution and geometric factor\n",
    "        efficiency[i] = np.trapz(nz[mask_behind] * (z_array[mask_behind] - zl) / z_array[mask_behind], \n",
    "                               z_array[mask_behind])\n",
    "\n",
    "ax4.plot(z_lens, efficiency / np.max(efficiency), 'purple', linewidth=2)\n",
    "ax4.set_xlabel('Lens Redshift z_lens')\n",
    "ax4.set_ylabel('Lensing Efficiency (normalized)')\n",
    "ax4.set_title('Lensing Efficiency Function')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n🔍 Key Insights:\")\n",
    "print(f\"  • Peak lensing efficiency occurs at z_lens ≈ {z_lens[np.argmax(efficiency)]:.2f}\")\n",
    "print(f\"  • This is roughly half the mean source redshift ({means[selected_bin]:.3f})\")\n",
    "print(f\"  • Lensing is most effective when lenses are at intermediate distances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Computing Angular Power Spectra\n",
    "\n",
    "The angular power spectrum C_ℓ describes the statistical properties of the convergence field. It's fundamental for:\n",
    "\n",
    "1. **Theoretical predictions** of lensing signals\n",
    "2. **Map generation** using tools like `hp.synfast()`\n",
    "3. **Cosmological parameter estimation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up angular scales (multipoles)\n",
    "ell = np.arange(2, 10000)  # Angular multipoles\n",
    "\n",
    "print(f\"🔢 Computing angular power spectrum:\")\n",
    "print(f\"  Multipole range: ℓ = {ell.min()} to {ell.max()}\")\n",
    "print(f\"  Angular scales: {180/ell.max():.3f}° to {180/ell.min():.1f}°\")\n",
    "\n",
    "if CCL_AVAILABLE and lens_tracer is not None:\n",
    "    # Compute the angular power spectrum for weak lensing\n",
    "    print(f\"\\n🔄 Computing C_ℓ^κκ using CCL...\")\n",
    "    cl_kappa = ccl.angular_cl(cosmo, lens_tracer, lens_tracer, ell)\n",
    "    \n",
    "    print(f\"✅ Power spectrum computed successfully\")\n",
    "    print(f\"  C_ℓ range: {cl_kappa.min():.2e} to {cl_kappa.max():.2e}\")\n",
    "    \n",
    "    # Save the power spectrum\n",
    "    output_filename = f'cl_kappa_mean_{means[selected_bin]:.2f}.txt'\n",
    "    np.savetxt(output_filename, np.column_stack((ell, cl_kappa)), \n",
    "               header=f'ell C_l_kappa for redshift bin {selected_bin+1}, mean z = {means[selected_bin]:.3f}')\n",
    "    print(f\"💾 Saved power spectrum to {output_filename}\")\n",
    "    \n",
    "else:\n",
    "    # Load existing power spectrum or create a model\n",
    "    try:\n",
    "        print(f\"\\n📁 Loading existing power spectrum from cl_kappa.txt...\")\n",
    "        cl_kappa_data = np.loadtxt(\"cl_kappa.txt\")\n",
    "        \n",
    "        # Interpolate to our ell range if needed\n",
    "        if len(cl_kappa_data) != len(ell):\n",
    "            from scipy.interpolate import interp1d\n",
    "            ell_file = np.arange(2, len(cl_kappa_data) + 2)\n",
    "            interp_func = interp1d(ell_file, cl_kappa_data, \n",
    "                                 bounds_error=False, fill_value='extrapolate')\n",
    "            cl_kappa = interp_func(ell)\n",
    "        else:\n",
    "            cl_kappa = cl_kappa_data\n",
    "        \n",
    "        print(f\"✅ Loaded power spectrum with {len(cl_kappa)} multipoles\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ cl_kappa.txt not found. Creating model power spectrum...\")\n",
    "        \n",
    "        # Create a realistic model power spectrum\n",
    "        # This is a simplified model for demonstration\n",
    "        ell_pivot = 1000\n",
    "        amplitude = 1e-7\n",
    "        slope = -1.5\n",
    "        \n",
    "        cl_kappa = amplitude * (ell / ell_pivot) ** slope\n",
    "        print(f\"✅ Created model power spectrum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Angular Power Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the angular power spectrum\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Power spectrum in log-log scale\n",
    "ax1.loglog(ell, cl_kappa, 'b-', linewidth=2, label='C_ℓ^κκ')\n",
    "ax1.set_xlabel('Multipole ℓ')\n",
    "ax1.set_ylabel('C_ℓ^κκ')\n",
    "ax1.set_title('Convergence Angular Power Spectrum')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Add angular scale annotations\n",
    "angular_scales = [1, 10, 60]  # degrees, arcminutes\n",
    "for scale in angular_scales:\n",
    "    ell_scale = 180 / scale\n",
    "    if ell_scale >= ell.min() and ell_scale <= ell.max():\n",
    "        ax1.axvline(ell_scale, color='red', linestyle='--', alpha=0.7)\n",
    "        ax1.text(ell_scale, cl_kappa.max(), f'{scale}°', rotation=90, \n",
    "                fontsize=10, color='red', ha='right')\n",
    "\n",
    "# Multipole times power spectrum (shows contribution to variance)\n",
    "ax2.semilogx(ell, ell * (ell + 1) * cl_kappa / (2 * np.pi), 'g-', linewidth=2)\n",
    "ax2.set_xlabel('Multipole ℓ')\n",
    "ax2.set_ylabel('ℓ(ℓ+1)C_ℓ^κκ / 2π')\n",
    "ax2.set_title('Power Spectrum (variance weighted)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some key statistics\n",
    "peak_ell = ell[np.argmax(ell * (ell + 1) * cl_kappa)]\n",
    "peak_scale = 180 / peak_ell\n",
    "\n",
    "print(f\"\\n📊 Power Spectrum Analysis:\")\n",
    "print(f\"  Peak contribution at ℓ ≈ {peak_ell:.0f}\")\n",
    "print(f\"  Corresponding angular scale: {peak_scale:.2f}°\")\n",
    "print(f\"  Total power (ℓ=2-1000): {np.trapz(cl_kappa[:999], ell[:999]):.2e}\")\n",
    "\n",
    "# Print information about the redshift bin\n",
    "print(f\"\\n🎯 This power spectrum is for:\")\n",
    "print(f\"  Redshift bin: Component {selected_bin + 1}\")\n",
    "print(f\"  Mean redshift: {means[selected_bin]:.3f}\")\n",
    "print(f\"  Galaxy population: {np.sum(bin_mask):,} galaxies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Cross-Bin Analysis\n",
    "\n",
    "Real weak lensing analyses often look at correlations between different redshift bins. This provides additional cosmological information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all redshift bins\n",
    "print(f\"🔍 Cross-Bin Analysis for All Components:\")\n",
    "print(f\"{'Bin':<5} {'Mean z':<8} {'N_gal':<8} {'Fraction':<10} {'z_eff':<8}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "bin_info = []\n",
    "for i in range(n_components):\n",
    "    mask = labels == i\n",
    "    n_gal = np.sum(mask)\n",
    "    fraction = n_gal / len(labels) * 100\n",
    "    z_eff = np.mean(z_data[mask]) if n_gal > 0 else 0\n",
    "    \n",
    "    print(f\"{i+1:<5} {means[i]:<8.3f} {n_gal:<8,} {fraction:<10.1f}% {z_eff:<8.3f}\")\n",
    "    \n",
    "    bin_info.append({\n",
    "        'bin': i+1,\n",
    "        'mean_z': means[i],\n",
    "        'n_galaxies': n_gal,\n",
    "        'fraction': fraction,\n",
    "        'z_effective': z_eff\n",
    "    })\n",
    "\n",
    "# Create redshift distribution plot for all bins\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# All bins on one plot\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "for i in range(n_components):\n",
    "    mask = labels == i\n",
    "    if np.sum(mask) > 0:\n",
    "        ax1.hist(z_data[mask], bins=40, alpha=0.6, density=True,\n",
    "                color=colors[i % len(colors)], \n",
    "                label=f'Bin {i+1}: z̄={means[i]:.3f}')\n",
    "\n",
    "ax1.set_xlabel('Redshift z')\n",
    "ax1.set_ylabel('Normalized n(z)')\n",
    "ax1.set_title('All Redshift Bins')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Lensing efficiency comparison\n",
    "z_lens_range = np.linspace(0, 2.0, 50)\n",
    "\n",
    "for i in range(min(3, n_components)):  # Show first 3 bins to avoid clutter\n",
    "    mask = labels == i\n",
    "    if np.sum(mask) > 100:  # Only bins with sufficient galaxies\n",
    "        z_sources = z_data[mask]\n",
    "        efficiency = []\n",
    "        \n",
    "        for zl in z_lens_range:\n",
    "            # Simple lensing efficiency calculation\n",
    "            sources_behind = z_sources[z_sources > zl]\n",
    "            if len(sources_behind) > 0:\n",
    "                # Geometric lensing efficiency\n",
    "                eff = np.mean((sources_behind - zl) / sources_behind)\n",
    "            else:\n",
    "                eff = 0\n",
    "            efficiency.append(eff)\n",
    "        \n",
    "        efficiency = np.array(efficiency)\n",
    "        if np.max(efficiency) > 0:\n",
    "            efficiency = efficiency / np.max(efficiency)  # Normalize\n",
    "        \n",
    "        ax2.plot(z_lens_range, efficiency, colors[i % len(colors)], \n",
    "                linewidth=2, label=f'Bin {i+1} (z̄={means[i]:.3f})')\n",
    "\n",
    "ax2.set_xlabel('Lens Redshift z_lens')\n",
    "ax2.set_ylabel('Lensing Efficiency (normalized)')\n",
    "ax2.set_title('Lensing Efficiency for Different Source Bins')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n💡 Cross-bin correlations allow us to:\")\n",
    "print(f\"  • Test cosmological models with multiple distance scales\")\n",
    "print(f\"  • Reduce systematic errors through redundancy\")\n",
    "print(f\"  • Improve parameter constraints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "1. **Redshift Analysis**: Extracted and analyzed galaxy redshift distributions\n",
    "2. **GMM Modeling**: Used Gaussian Mixture Models to identify redshift populations\n",
    "3. **Cosmological Setup**: Configured PyCC for lensing calculations\n",
    "4. **Power Spectra**: Computed angular power spectra for different redshift bins\n",
    "5. **Cross-bin Analysis**: Explored correlations between different galaxy populations\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Redshift distributions** n(z) describe galaxy distance distributions\n",
    "- **Gaussian Mixture Models** can decompose complex redshift distributions\n",
    "- **Lensing efficiency** depends on the geometry of lens-source configurations\n",
    "- **Angular power spectra** encode the statistical properties of convergence fields\n",
    "- **Cross-bin correlations** provide additional cosmological information\n",
    "\n",
    "### Next Tutorial Preview\n",
    "\n",
    "In **Tutorial 4: Power Spectra and Convergence Maps**, you will learn:\n",
    "- How to generate convergence maps from power spectra\n",
    "- Understanding different map-making techniques (Gaussian vs lognormal)\n",
    "- Validating generated maps against theoretical expectations\n",
    "- Preparing convergence maps for forward modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial completion summary\n",
    "print(\"📋 TUTORIAL 3 COMPLETED\")\n",
    "print(\"=\" * 50)\n",
    "print(\"✅ Analyzed galaxy redshift distributions\")\n",
    "print(\"✅ Implemented Gaussian Mixture Model decomposition\")\n",
    "print(\"✅ Set up cosmological models with PyCC\")\n",
    "print(\"✅ Computed angular power spectra\")\n",
    "print(\"✅ Performed cross-bin lensing efficiency analysis\")\n",
    "print(\"✅ Generated power spectra for different redshift bins\")\n",
    "\n",
    "print(f\"\\n📊 Results Summary:\")\n",
    "print(f\"  • {n_components} redshift components identified\")\n",
    "print(f\"  • Primary component: z̄ = {means[selected_bin]:.3f} ({np.sum(labels==selected_bin):,} galaxies)\")\n",
    "print(f\"  • Power spectrum: ℓ = {ell.min()}-{ell.max()}, C_ℓ range: {cl_kappa.min():.2e}-{cl_kappa.max():.2e}\")\n",
    "\n",
    "if CCL_AVAILABLE:\n",
    "    print(f\"  • Full cosmological modeling with PyCC\")\n",
    "else:\n",
    "    print(f\"  • Demonstrated workflow (install PyCC for full functionality)\")\n",
    "\n",
    "print(\"\\n🚀 Ready for Tutorial 4: Power Spectra and Convergence Map Generation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}